{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "#from pandas_ml import ConfusionMatrix as CM\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\data scientist\\kaggle dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading dataset and converting to array\n",
    "train_data=pd.read_csv(\"train.csv\")#.values.astype('int32')\n",
    "test_data=pd.read_csv(\"test.csv\")#.values.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"label\"]  # store target value coresponding to pixels present\n",
    "\n",
    "x_train = train_data.drop(labels = [\"label\"] , axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some Pre-Processing \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f517b31860>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f5187c94e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(y_train)\n",
    "#(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check is there any missing value present\n",
    "\n",
    "x_train.isnull().sum()\n",
    "x_train.isnull().any().describe()\n",
    "\n",
    "test_data.isnull().sum()\n",
    "test_data.isnull().any().describe()  #no missing value present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize/Scaling  the data which is range (0-1)--- to do that use normalize function which is in tensorflow keras\n",
    "# another way to normalize the data is simple divive by 255\n",
    "\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "test_data = tf.keras.utils.normalize(test_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-5bf06d532448>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Reshape image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#y_train = tf.keras.utils.to_categorical(y_train, num_classes = 10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "#Reshape image \n",
    "x_train = x_train.values.reshape(-1,28,28,1)\n",
    "test_data = test_data.values.reshape(-1,28,28,1)\n",
    "\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and evaluate sets\n",
    "# Lets choose to split the train set in two parts \n",
    "# A small fraction (10%) became the validation set which the model is evaluated and the rest (90%) is used to train the model.\n",
    "\n",
    "\n",
    "x_train_data, x_eval, y_train_data, y_eval = tts(x_train, y_train, test_size = 0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21250    2\n",
       "20334    3\n",
       "29647    2\n",
       "3220     9\n",
       "5293     0\n",
       "18876    7\n",
       "19344    3\n",
       "1268     1\n",
       "25937    8\n",
       "25624    5\n",
       "23405    7\n",
       "25031    1\n",
       "26378    8\n",
       "26250    7\n",
       "37335    4\n",
       "13154    8\n",
       "17153    2\n",
       "26110    4\n",
       "25707    7\n",
       "14624    7\n",
       "20098    9\n",
       "13982    0\n",
       "23526    3\n",
       "2854     0\n",
       "37586    9\n",
       "14357    2\n",
       "1700     4\n",
       "2077     0\n",
       "39586    5\n",
       "24823    7\n",
       "        ..\n",
       "12420    3\n",
       "19694    8\n",
       "36432    4\n",
       "16415    8\n",
       "27859    1\n",
       "40390    4\n",
       "25902    0\n",
       "16639    2\n",
       "21372    2\n",
       "20026    6\n",
       "15905    9\n",
       "8170     9\n",
       "29547    1\n",
       "21418    9\n",
       "20084    1\n",
       "41084    2\n",
       "6548     1\n",
       "32031    5\n",
       "11071    9\n",
       "5167     1\n",
       "35541    8\n",
       "33201    9\n",
       "14696    4\n",
       "33867    2\n",
       "18898    0\n",
       "31019    7\n",
       "30280    9\n",
       "6637     2\n",
       "35343    9\n",
       "23720    4\n",
       "Name: label, Length: 33600, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "33600/33600 [==============================] - 1237s 37ms/step - loss: 0.2362 - acc: 0.9232\n",
      "Epoch 2/2\n",
      "33600/33600 [==============================] - 1234s 37ms/step - loss: 0.0694 - acc: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f530541518>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the model\n",
    "# with the help of keras sequential we built a model, in this we add one layer at a time starting with flatten input\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Flatten()) # just reshaping the pixel as single line input\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation =tf.nn.relu, input_shape = (28,28,1)))\n",
    "model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation =tf.nn.relu))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation =tf.nn.relu))\n",
    "model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation =tf.nn.relu))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation= tf.nn.relu)) # 128 is no of neurons present in this layer\n",
    "model.add(tf.keras.layers.Dense(128, activation= tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation= tf.nn.softmax))  # output layer\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_data, y_train_data, epochs=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400/8400 [==============================] - 21s 2ms/step\n",
      "0.0551207129247 0.983214285714\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model i.e how accurate our model works\n",
    "\n",
    "val_los, val_acc = model.evaluate(x_eval, y_eval)\n",
    "print(val_los, val_acc) # loss = .055 &  Accuracy = .98\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save(\"digit_recognize.model\")\n",
    "#loading the model to predict\n",
    "model_load = tf.keras.models.load_model(\"digit_recognize.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction of test data\n",
    "predictions = model_load.predict([test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_values= np.argmax(predictions, axis = 1)\n",
    "arg_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_values = pd.Series(arg_values,name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),arg_values],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_file.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
